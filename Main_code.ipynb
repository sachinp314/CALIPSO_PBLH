{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fea1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tables\n",
    "from pyhdf import SD\n",
    "import h5py\n",
    "import os\n",
    "import datetime as dt\n",
    "import pyhdf.VS\n",
    "import pyhdf\n",
    "from pyhdf.HDF import HDF\n",
    "import scipy.signal\n",
    "import math\n",
    "\n",
    "#for mapping\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4950206",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables.file._open_files.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1ef4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\E Contents'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check path and ensure you are in path nearby data folder\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7208d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter your path followed by folder name and print once to make sure only hdf files are there in folder, then proceed\n",
    "folderpath = r\"D:\\E Contents\\ADHM 3 months\" # make sure to put the 'r' in front\n",
    "filepaths  = [os.path.join(folderpath, name) for name in os.listdir(folderpath)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907bfd4b",
   "metadata": {},
   "source": [
    "### Extraction Function\n",
    "Calipso files are each approximately 400-500Mb for one transit. So given a time window there are multiple files. Keep all the files in one particular folder after downloading from the CALIPSO Earthdata search.\n",
    "\n",
    "Using this function, for each file we extract the metadata and set index as the date and time at start of granule.\n",
    "Then we extract the parameters. Some of the parameters may be multidimensional i.e. for each timestamp they will have multiple readings, for eg. Backscatter_532 for each timestamp has 583 values corresponding to different altitudes. This altitude data is given in the metadata. For multidimensional parameters we convert them into a dataframe and store the whole file as dataframe within dataframe. We also specificy the latitude and longitude box for data and truncate it. This is the primary step in extraction as each CALIPSO granule has all lat and long for a given transit, and its necessary to discard redundant data to save space.\n",
    "\n",
    "We then run a loop to extract all the files, and save them in a newly created HDF file to access data much more easily. We append main data to the hdf file itself at each loop. We use the corresnponding key from metadata to be the granule key in our multi-index dataframe. For saving the first set of data make sure to delete other file for previous names(do this while overwriting same data again too). If you are downloading new data and want to append the data then remove the delete file line and continue appending. Metadata files are appended as dataframes and together converted to HDF again to save space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469238a",
   "metadata": {},
   "source": [
    "For example I downloaded files covering the latitudes and longitudes of Ali, Devasthal, Hanle and merak for 6 month (over 30 granules when satellite passes over), and we extract the granules and truncate it to those coordinates and save it as truncated dataset and metadata. In the instance that you download and want to process the granules separately, i.e. 20 first then 20 next to save computing power, then use the append mode (Type 'a'). Otherwise use the write mode (Type 'w'). This will automatically load existing metadata also to append, whereas writing for the first time will delete previous written files of same name. Please be careful. Kindly change the file names accordingly before writing a new file.\n",
    "\n",
    "To extract other HDF files that are not CALIPSO (since we have explicitly used column names and modified data), it is better to use SD.SD from pyhdf, and HDF from pyhDF.HDF to access the columns and fields in data and metadata. The documentation is provided online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a7e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables.file._open_files.close_all()\n",
    "\n",
    "def extract_meta(file_name):\n",
    "    #Access Metadata methodology\n",
    "    #create a HDF interface\n",
    "    HDF_interface = HDF(file_name)\n",
    "\n",
    "    #initialize the VS API over the file and return a VS instance using the vstart() function of the HDF interface.\n",
    "\n",
    "    vs_interface = HDF_interface.vstart()\n",
    "\n",
    "    #Next, I use attach() to retrieve the \"metadata\" VD instance.\n",
    "\n",
    "    meta = vs_interface.attach(\"metadata\")\n",
    "    #Retrieve info about all vdata fields.\n",
    "    field_infos = meta.fieldinfo()\n",
    "    #retreieve all data values\n",
    "    all_data = meta.read(meta._nrecs)[0]\n",
    "\n",
    "    meta.detach() #terminate access to vdata\n",
    "    \n",
    "    #put everthing in a dictionary\n",
    "    metadata_dictionary = {}\n",
    "    field_name_index = 0\n",
    "    for field_info, data in zip(field_infos, all_data):\n",
    "        metadata_dictionary[field_info[field_name_index]] = data\n",
    "        \n",
    "        \n",
    "    metadata_df = pd.Series(metadata_dictionary).to_frame().T #insert the metadata dictionary as rows i.e. columns will be diff attributes\n",
    "    metadata_df.set_index('Date_Time_at_Granule_Start', inplace=True)\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "def extract_func(file,trunc_list):\n",
    "    ext_SD = SD.SD(file)\n",
    "    para_list = list(ext_SD.datasets().keys())\n",
    "\n",
    "    #extract the file and its keys to form a dictionary.\n",
    "    data_dict = {}\n",
    "    for keys in para_list:\n",
    "        data_dict[keys] = ext_SD.select(keys).get()\n",
    "\n",
    "    #if any data is mutidimensional convert it into a dataframe with the suffix _df\n",
    "    data_df_dict = {}\n",
    "    for key in data_dict.keys():\n",
    "        if len(data_dict[key][0]) != 1:\n",
    "            data_df_dict[key + \"_df\"]= [pd.DataFrame(obj,columns=['val']) for obj in data_dict[key]]\n",
    "\n",
    "\n",
    "    data_dict_flat = {}\n",
    "    for key in data_dict.keys():\n",
    "        if len(data_dict[key][0]) == 1:\n",
    "            data_dict_flat[key] = data_dict[key].flatten()\n",
    "\n",
    "    data_dict_final = data_dict_flat.copy()\n",
    "    data_dict_final.update(data_df_dict)\n",
    "\n",
    "    df_final = pd.DataFrame({k:list(v) for k,v in data_dict_final.items()})\n",
    "    \n",
    "    metadata_df = extract_meta(file)\n",
    "\n",
    "    ###Truncation\n",
    "    \n",
    "    \n",
    "    data_long_trunc = df_final[df_final['Longitude']>trunc_list[1]]\n",
    "    data_long_trunc = data_long_trunc[data_long_trunc['Longitude']<trunc_list[3]]\n",
    "    \n",
    "    data_lat_trunc = data_long_trunc[data_long_trunc['Latitude']>trunc_list[0]]\n",
    "    data_lat_trunc = data_lat_trunc[data_lat_trunc['Latitude']<trunc_list[2]]\n",
    "    \n",
    "    sizelist = [len(df_final),len(data_long_trunc),len(data_lat_trunc)]\n",
    "    \n",
    "#     data_lat_trunc['Total_Attenuated_Backscatter_532']['Altitudes'] = metadata_df['Lidar_Data_Altitudes']\n",
    "    \n",
    "    \n",
    "    data_lat_trunc.reset_index(inplace=True)\n",
    "    data_lat_trunc.drop(\"index\",axis=1,inplace=True)\n",
    "\n",
    "    return data_lat_trunc,metadata_df,sizelist\n",
    "\n",
    "# Apart from extraction we need to extract and append accordingly based on the structure. data files appended with index\n",
    "# whereas the metadata is appended together\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    a_w_variable = input(\"Enter 'a' for append, 'w' for write:\")\n",
    "    \n",
    "    if a_w_variable == 'w':\n",
    "        metadata_df = pd.DataFrame()\n",
    "        filepath = os.path.join(os.getcwd(),\"FinalData_truncated.hdf\")\n",
    "        if os.path.exists(filepath) == True:\n",
    "            os.remove(filepath)\n",
    "    if a_w_variable == 'a':\n",
    "        filepath = os.path.join(os.getcwd(),\"Final_metadata.hdf\")\n",
    "        if os.path.exists(filepath) == True:\n",
    "            metadata_df = pd.read_hdf(\"Final_metadata.hdf\",'metadata')\n",
    "    else:\n",
    "        a_w_variable = input(\"Enter 'a' for append, 'w' for write:\")\n",
    "        \n",
    "    folderpath = input(\"Enter data folder path:\")\n",
    "    print(folderpath)\n",
    "\n",
    "    file_list = [os.path.join(folderpath, name) for name in os.listdir(folderpath)]\n",
    "       \n",
    "    coord_list = [float(item) for item in input(\"Enter coordinates (order: minlat,minlong,maxlat,maxlong):\").split(',')]\n",
    "    \n",
    "#     if the datafile exists then you will be increasing size in append mode even though with the same key the data\n",
    "#     is overwritten (key deleted and written), hdf5 does not reclaim space so the size of the file keeps increasing.\n",
    "\n",
    "        \n",
    "    sizelist = []\n",
    "    \n",
    "    for filename in file_list:\n",
    "        print(filename, end='\\r')      \n",
    "    # call the extrtion function on one file granule and then append them one by one using metadata key\n",
    "        data_trunc_temp, metadata_temp,ind_sizelist = extract_func(filename,coord_list)\n",
    "    #     data_trunc_df = pd.concat([data_trunc_df,data_trunc_temp], axis = 0,join='outer')\n",
    "\n",
    "        metadata_df = pd.concat([metadata_df,metadata_temp], axis = 0,join='outer')\n",
    "        data_trunc_temp.to_hdf(\"FinalData_truncated.hdf\",metadata_temp.index[0],mode='a')\n",
    "        sizelist.append(ind_sizelist)\n",
    "        print(ind_sizelist)\n",
    "\n",
    "    metadata_df.to_hdf(\"Final_metadata.hdf\",'metadata',mode='w')\n",
    "    print(sizelist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e51a3f",
   "metadata": {},
   "source": [
    "### Read Extracted Data\n",
    "Whatever is stored in the truncated data and metadata file can be read now. While reading the data itself the code performs certain operations like appending the altitude columns from metadata file into the Total_attenuated_backscatter_532, and we convert the datetime from CALIPSO format (in UTC) which starts from 1993 Jan 1 to readable datetime. \n",
    "\n",
    "Then we shuffle the dataframe and bring forward the things needed mainly like latitude, longitude, TAB_532, surface elevation. We also use parameters such as 'Number_Bins_Shift' and 'Shifted_Altitude' to correspondingly shift our altitude column for each data point. Our hdf files will now be transformed into easily accessible dataframe.\n",
    "\n",
    "The code goes through each item in metadata index, which is the index key for HDF5 file of the truncated data, and we use pandas to read the HDF5 one by one. We print this index as we loop through them. Later when shifting altidudes we iterate through rows and print the index from iterrows to monitor code flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2184f5ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 60>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m temp_df,metadata_df\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 61\u001b[0m     data_df,metadata_df \u001b[38;5;241m=\u001b[39m \u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFinalData_truncated.hdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFinal_metadata.hdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mread_data\u001b[1;34m(datafile, metadatafile)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_data\u001b[39m(datafile,metadatafile):\n\u001b[1;32m----> 2\u001b[0m     temp_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m      4\u001b[0m     metadata_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_hdf(metadatafile,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend_alt\u001b[39m(x):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def read_data(datafile,metadatafile):\n",
    "    temp_df = pd.DataFrame()\n",
    "\n",
    "    metadata_df = pd.read_hdf(metadatafile,'metadata')\n",
    "    \n",
    "    def append_alt(x):\n",
    "        x['Altitudes'] = metadata_df['Lidar_Data_Altitudes'].values.tolist()[0]\n",
    "    \n",
    "    def Converdate(inputtime):\n",
    "        conv = dt.datetime(1993,1,1,0,0,0,0) + dt.timedelta(0,inputtime)\n",
    "        return conv.isoformat()\n",
    "\n",
    "    ###when you append specify groupname and use groupby then simpsons/trap scipy integration. (that is for avergaing?)\n",
    "    ###for intergrating one df guy and put it as new column and store. simple. \n",
    "    \n",
    "    \n",
    "    \n",
    "    for item in metadata_df.index:\n",
    "        \n",
    "        test = pd.read_hdf(datafile,item) #reading we have to specify filename and the particular key if multiple data is appended\n",
    "        test['SrNo'] = test.index\n",
    "        test['granule'] = [item.split('T')[0]]*len(test)\n",
    "        test.set_index(['granule','SrNo'],inplace=True)\n",
    "        \n",
    "        temp_df = pd.concat([temp_df,test],axis=0,join='outer')\n",
    "        print(item)\n",
    "    \n",
    "    \n",
    "    #Placing important columns at the start\n",
    "    temp = temp_df.pop('Total_Attenuated_Backscatter_532_df')\n",
    "    temp_df.insert(0,'TAB_532', temp)\n",
    "    \n",
    "    \n",
    "    temp_df['TAB_532'].apply(lambda x : append_alt(x)) #appends altitude array to each TAB dataframe\n",
    "    \n",
    "    temp_df['date_time'] = temp_df['Profile_Time'].apply(lambda x : Converdate(x))\n",
    "    \n",
    "    temp = temp_df.pop('date_time')\n",
    "    temp_df.insert(0,'date_time', temp)\n",
    "    \n",
    "    \n",
    "    temp = temp_df.pop('Latitude')\n",
    "    temp_df.insert(1,'Latitude', temp)\n",
    "    \n",
    "    \n",
    "    temp = temp_df.pop('Longitude')\n",
    "    temp_df.insert(2,'Longitude', temp)\n",
    "    \n",
    "    \n",
    "    #shifting the appended altitude array with number bins shift and surface altitude shift based on their definition given in\n",
    "    #the CALIPSO documentation, for each datapoint the altitudes within TAB_532 will be correspondingly shifted\n",
    "    #based on these two columns given in Level 1 Data CALIPSO\n",
    "    shifted_altitudes = []\n",
    "\n",
    "    for row in temp_df[['TAB_532','Number_Bins_Shift','Surface_Altitude_Shift']].iterrows():\n",
    "        print(row[0],end='\\r')\n",
    "        row[1]['TAB_532']['Altitudes'] = row[1]['TAB_532']['Altitudes'].shift(periods=row[1]['Number_Bins_Shift'])\n",
    "        row[1]['TAB_532']['Altitudes'] = row[1]['TAB_532']['Altitudes'].apply(lambda x: x + row[1]['Surface_Altitude_Shift'])\n",
    "#         print(row[1]['TAB_532'])\n",
    "        temp_df.at[row[0],'TAB_532'] = row[1]['TAB_532']\n",
    "\n",
    "    return temp_df,metadata_df\n",
    "\n",
    "\n",
    "#The function is called to read truncated dataset and metadata in data_df and metadata_df\n",
    "if __name__ == \"__main__\":\n",
    "    data_df,metadata_df = read_data('FinalData_truncated.hdf','Final_metadata.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d67d23d",
   "metadata": {},
   "source": [
    "\n",
    "#### This is a function to use bit encoded data in QC_Flag_2 parameter of CALIPSO to ensure we only select good data.\n",
    "We form a new list called GB_list(Good/Bad), we convert the quality checks into 32bit string and check if any 1 exists \n",
    "in 11th to 16th bit (read left to right that is why we reverse), if yes we reject data (GB_list val is 1), then we check for 1's in consecutive timestamps i.e. 1250 timestamps. If a 1 in the bitstring (at any position) persists for many minutes then it is bad data. So we check how many integer value 0 are there in the next 1250 data and if 0 not in it then we reject data (GB_list value is 1). If none of this condition satisfied we accept data (GB_list val is 0). We are quality checking for approximately half a minute, less than the several minutes condition. One minute wil be 1250 other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7800734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_bad(qc_flag2):\n",
    "#     print(range(len(qc_flag2)))\n",
    "    GB_list = []\n",
    "#     print(GB_list)\n",
    "#     print(qc_flag2)\n",
    "    qc_bits = list(map(lambda x:'{:032b}'.format(x),qc_flag2.values))\n",
    "#     print(len(qc_bits))\n",
    "#     print(qc_flag2.index.get_level_values(1))\n",
    "\n",
    "    bandwith = 500\n",
    "    for i in qc_flag2.index.get_level_values(1):\n",
    "#         print(qc_flag2[i])\n",
    "        bit_rev = '{:032b}'.format(qc_flag2[i])[::-1]\n",
    "#         print(i,bit_rev)\n",
    "        if '1' in bit_rev[10:15]:\n",
    "            GB_list.append(1)\n",
    "            continue\n",
    "        if '1' in qc_bits[i]:\n",
    "#             print(qc_bits[i])\n",
    "            end_range = min(i+bandwith,len(qc_flag2)) #the next 500 timestamps or length of list whichever shorter is investigated\n",
    "#             print(i,end_range)\n",
    "            check_list_num = list(qc_flag2[i:end_range].values)\n",
    "#             print(check_list_num)\n",
    "            \n",
    "            if 0 in check_list_num:\n",
    "                GB_list.append(0)\n",
    "            else:\n",
    "                GB_list.append(1)\n",
    "        else:\n",
    "            GB_list.append(0)\n",
    "\n",
    "    return GB_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66200add",
   "metadata": {},
   "source": [
    "#### Averaging the backscatter data\n",
    "Function that averages a singledate file given a dataframe and binsize in lat/longitude degrees for averaging\n",
    "Horizontal averagin to remove SNR.\n",
    "Given a binsize(in latitude degrees), we subtract and add that to the minimum and maximum values of latitude. Then we make a list of tuples containing latitude interval bins and make it the Interval Index of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd7fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def group_hor_mean(df_singledate, binsize):\n",
    "#     binrange = (30.5,32.5)\n",
    "#     binsize = 0.25\n",
    "\n",
    "    def custom_round(x, base):\n",
    "        return base * round(float(x)/base)\n",
    "\n",
    "# you can replace the base parameter with the nearest rounding you need  \n",
    "#     print(len(df_singledate))\n",
    "\n",
    "    \n",
    "    \n",
    "#     print(df_singledate['Latitude'].min(),df_singledate['Latitude'].max())\n",
    "\n",
    "# Creating a bin list with given binsize that will be used to create an interval index\n",
    "    a = custom_round(df_singledate['Latitude'].min(),binsize)\n",
    "    b = custom_round(df_singledate['Latitude'].max(),binsize)\n",
    "    \n",
    "    if (a>df_singledate['Latitude'].min()):\n",
    "        a = a - binsize\n",
    "    \n",
    "    if (b<df_singledate['Latitude'].max()):\n",
    "        b = b + binsize\n",
    "        \n",
    "        \n",
    "    bins = []\n",
    "    temp = float('-inf')\n",
    "    while (temp < b):\n",
    "        temp = a + binsize\n",
    "        bins.append((a,temp))\n",
    "        a = temp\n",
    "    \n",
    "#     print(bins)\n",
    "    list_of_bins = pd.IntervalIndex.from_tuples(bins) #make interval index from bins list\n",
    "    \n",
    "#     print(list_of_bins)\n",
    "    \n",
    "    \n",
    "    df_singledate['Latitude_bins'] = pd.cut(df_singledate['Latitude'], list_of_bins) #segments or sorts data values into bins\n",
    "    df_singledate = df_singledate.dropna(subset = ['Latitude_bins'])\n",
    "  \n",
    "\n",
    "    list_of_means = []\n",
    "    nodata_list = []\n",
    "#     s = 0\n",
    "    #groupby and average data based on their latitude bins column\n",
    "    for group_name, group_data in df_singledate.groupby(['Latitude_bins']):\n",
    "        #some datapoints may be removed in quality check, hence those datapoints are not considered\n",
    "        if(group_data['TAB_532'].shape[0] == 0): \n",
    "            nodata_list.append(group_name)\n",
    "            continue    #skip this loop \n",
    "#         s = s + len(group_data)\n",
    "# We take averages of everything when grouped with latitude bins and then multiply that \n",
    "        long_list = [long for long in group_data['Longitude']]\n",
    "        avg_long = np.average(long_list)\n",
    "        lat_list = [lat for lat in group_data['Latitude']]\n",
    "        avg_lat = np.average(lat_list)\n",
    "        \n",
    "       \n",
    "        DN_list= [DN for DN in group_data['Day_Night_Flag']]\n",
    "        DN_flag = np.average(DN_list)\n",
    "        \n",
    "        surf_ele = [elev for elev in group_data['Surface_Elevation']]\n",
    "        avg_ele = np.average(surf_ele)\n",
    "        \n",
    "        dt_length = len(group_data['date_time'])\n",
    "        avg_datetime = dt.datetime.fromtimestamp(sum(map(dt.datetime.timestamp,map(dt.datetime.fromisoformat,group_data['date_time']))) / dt_length).isoformat()\n",
    "#         print(avg_datetime)\n",
    "        \n",
    "        list_TAB_df = pd.concat(group_data['TAB_532'].values)\n",
    "        mean_df = list_TAB_df.groupby(list_TAB_df.index).mean()\n",
    "        lat_val = (group_name.left,group_name.right)\n",
    "        \n",
    "        mean_df['lat_bin_val'] = str(group_name)\n",
    "        mean_df.set_index(['lat_bin_val',mean_df.index],inplace=True)\n",
    "#         print(len(mean_df))\n",
    "# Then we create a mean_dataframe that contains lat_bin as the index, other fields are repeated and we extract TAB_532 and append\n",
    "# them as rows itself to later be useful when we use scipy peaks and have to do arithmeetic on it.\n",
    "        mean_df['date_time'] = [avg_datetime] * len(mean_df)\n",
    "        mean_df['Surface_elevation'] = [avg_ele]*len(mean_df)\n",
    "        \n",
    "        mean_df['Longitude'] = [avg_long] * len(mean_df)\n",
    "        mean_df['Latitude'] = [avg_lat] * len(mean_df)\n",
    "        mean_df['DN_Flag'] = [DN_flag]*len(mean_df)\n",
    "\n",
    "    \n",
    "        list_of_means.append(mean_df)\n",
    "\n",
    "    mean_TAB_532 = pd.concat(list_of_means)\n",
    "#     print(s)\n",
    "    return mean_TAB_532, nodata_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699dd472",
   "metadata": {},
   "source": [
    "Function to act on all files instead of singledate and return them in form a dataframe that contains averaged and quality\n",
    "checked data alone. The mean data will have 583 indexed rows (corresponding to TAB Altitudes), within each date (level 0 index) and latitude bin range(level 1 index). We go through each date and make bins and average accordingly and append 583 rows at each date and each bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_mean_df(data_df,binsize): \n",
    "    data_df['GB_2'] = is_bad(data_df['QC_Flag_2']) #prepares a GB_2 column from function is_bad defined above\n",
    "    list_of_means = []\n",
    "    no_data = []\n",
    "    empty_list = []\n",
    "    for item in data_df.index.get_level_values(0).unique():\n",
    "        print(len(data_df.loc[item]))\n",
    "        df_singledate = data_df.loc[item][data_df.loc[item]['QC_Flag']==0] #checks quality flag 1\n",
    "        print(len(df_singledate))\n",
    "        df_singledate = df_singledate[df_singledate['GB_2']==0] #checks and includes data that only satisfy this\n",
    "        print(len(df_singledate))\n",
    "\n",
    "#         df_singledate = data_df.loc[item]\n",
    "\n",
    "#         print(len(df_singledate))\n",
    "\n",
    "        if(len(df_singledate)==0): #this is to later make sure we dont have index correesponding to singledate with NULL objects\n",
    "            empty_list.append(item)\n",
    "            continue\n",
    "\n",
    "#         print(item)\n",
    "        mean_TAB_532,nodata = group_hor_mean(df_singledate,binsize) #mention the latitude averaging\n",
    "        list_of_means.append(mean_TAB_532)\n",
    "        no_data.append(nodata)\n",
    "    \n",
    "#     print(len([date for date in data_df.index.get_level_values(0).unique() if date not in empty_list]),len(list_of_means))\n",
    "    mean_QC = pd.concat(list_of_means, keys = [date for date in data_df.index.get_level_values(0).unique() if date not in empty_list])\n",
    "    #if there is a dataframe of a whole day that is removed by quality check we throw away the date to avoid appending null error\n",
    "                        \n",
    "    return mean_QC,no_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0256cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the printed values we can adjust the number to check consistent QC_Flag_2 in is_bad function. If too many datasets are \n",
    "#getting removed, increase the bandwith from 500. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d551787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the function on your data_df\n",
    "mean_TAB,no_data_list = get_mean_df(data_df,0.1)\n",
    "mean_TAB_2,no_data_list = get_mean_df(data_df,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b65c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do multiple averaging and save them in separate HDF to be read later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c48db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_TAB.to_hdf(\"QC_backscatter_mean_point_one.hdf\",'QC_mean',mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f70f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_TAB_2.to_hdf(\"QC_backscatter_mean_half_deg.hdf\",'mean',mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93429957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81dcb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bb5a1c6",
   "metadata": {},
   "source": [
    "#### Load saved averaged data ffor extraction of PBLH using MSD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bbba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_QC_half_deg = pd.read_hdf(\"Backscatter_mean_half_deg.hdf\",'QC_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_QC_point_one = pd.read_hdf(\"Backscatter_mean_point_one.hdf\",'QC_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3aa2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_QC_half_deg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa922ea",
   "metadata": {},
   "source": [
    "### Extracting PBLH\n",
    "\n",
    "To extract PBLH we look at only the altitudes which are relvant (300m from the surface to 5kms from the surface), we also use the data given to us by surface elevation to truncate each single_files (one date and one lat_bin) within that altitude range.\n",
    "\n",
    "We then take the standard deviation with four consecutives backscatter signals and altitude grouped. We identify the peaks corresponding to the backscatter signal and the standard deviation data. We then only consider important peaks defined with a cutoff height of the mean value of backscatter and standard deviation peaks. \n",
    "We then get the list of altitudes that correspond to these important peaks in backscatter as well as standard deviation (altitude values fot std is averaged for 4 points). These lists are also sorted, then we find for each altitude value of standard deivation maximum the corresponding nearest altitude value of the backscatter peak maximum. For every alt_val from standard deviation we find the difference of TAB value altitudes and store it in candidates list for PBLH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_PBLH(single_file):\n",
    "    \n",
    "    surf_ele = single_file['Surface_elevation'].iloc[0]\n",
    "    cloud_bin_factor = 10\n",
    "    \n",
    "    trunc_after=(single_file[single_file['Altitudes']<0].index[0]-(math.ceil(single_file['Surface_elevation'][0]/0.03))-cloud_bin_factor)\n",
    "    trunc_before = trunc_after - 160\n",
    "#160*333m ~ 5kms from the surface elevation value is the upper limit for identifying PBLH\n",
    "    single_file = single_file.truncate(before=trunc_before,after=trunc_after)\n",
    "    \n",
    "    std_list = pd.Series(single_file['val'].groupby(single_file.index // 4).std())\n",
    "    #max_bs_list = np.array(single_file['val'].groupby(single_file.index // 4).max())\n",
    "    alt_list = pd.Series(single_file['Altitudes']).groupby(single_file.index // 4).mean()\n",
    "    #alt_compare_list = pd.Series({group_name:group_data for group_name,group_data in single_file['Altitudes'].groupby(single_file.index // 4)})\n",
    "    \n",
    "    peaks, _ = scipy.signal.find_peaks(single_file['val'])\n",
    "    var_peaks, _ = scipy.signal.find_peaks(std_list)\n",
    "    \n",
    "    imp_peaks = scipy.signal.find_peaks(single_file['val'],height=np.mean(np.sort(single_file['val'].iloc[peaks])))\n",
    "    imp_var_peaks=scipy.signal.find_peaks(std_list,height=np.mean(np.sort(std_list.iloc[var_peaks])))\n",
    "    \n",
    "    TAB_peak_list = single_file['Altitudes'].iloc[imp_peaks[0]].iloc[np.argsort(single_file['val'].iloc[imp_peaks[0]])]\n",
    "    var_peak_list = alt_list.iloc[imp_var_peaks[0]].iloc[np.argsort(alt_list.iloc[imp_var_peaks[0]])]\n",
    "    \n",
    "    PB_val = None\n",
    "    \n",
    "    \n",
    "    candidate_PB_list = []\n",
    "    for alt_val in var_peak_list:\n",
    "#         left_range = alt_val - 0.12\n",
    "#         right_range = alt_val + 0.12\n",
    "        \n",
    "        candidate_PB_diff = []\n",
    "        for val in TAB_peak_list:\n",
    "            candidate_PB_diff.append((val,abs(alt_val-val)))\n",
    "\n",
    "        candidate_PB_list.append((alt_val,sorted(candidate_PB_diff, key = lambda x: x[1])[0]))\n",
    "#     print(candidate_PB_diff)\n",
    "         \n",
    "    \n",
    "#     print(sorted(candidate_PB_list, key = lambda x: x[1][0]))\n",
    "        \n",
    "    \n",
    "    PB_val = sorted(candidate_PB_list, key = lambda x: x[1][0])[0][1][0] #first guy in list, second value in tuple, first value in tuple\n",
    "                cc\n",
    "            \n",
    "    return (PB_val - surf_ele)\n",
    "                \n",
    "        \n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0633e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract PBLH of different datetimes. \n",
    "def extract_df_PB(mean_df):\n",
    "\n",
    "    PBLH_dict = {'date_time':[],'Lat':[],'Long':[],'PBLH':[],'Surface_elevation':[],'DN_Flag':[]}\n",
    "    for date in mean_df.index.get_level_values(0).unique():\n",
    "        for lat_bin in mean_df.loc[date].index.get_level_values(0).unique():\n",
    "            print(date,lat_bin)\n",
    "            PBLH = extract_PBLH(mean_df.loc[date].loc[lat_bin])\n",
    "            #print(PBLH)\n",
    "            PBLH_dict['PBLH'].append(PBLH)\n",
    "            PBLH_dict['date_time'].append(mean_df.loc[date].loc[lat_bin]['date_time'].iloc[0])\n",
    "            PBLH_dict['Lat'].append(mean_df.loc[date].loc[lat_bin]['Latitude'].iloc[0])\n",
    "            PBLH_dict['Long'].append(mean_df.loc[date].loc[lat_bin]['Longitude'].iloc[0])\n",
    "            PBLH_dict['Surface_elevation'].append(mean_df.loc[date].loc[lat_bin]['Surface_elevation'].iloc[0])\n",
    "            PBLH_dict['DN_Flag'].append(mean_df.loc[date].loc[lat_bin]['DN_Flag'].iloc[0])\n",
    "            \n",
    "    PBLH_df = pd.DataFrame(PBLH_dict)\n",
    "    return PBLH_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc675fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PBLH_df_half_deg = extract_df_PB(mean_QC_half_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PBLH_df_point_one = extract_df_PB(mean_QC_point_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc6d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "PBLH_df_point_one['date_time']=PBLH_df_point_one['date_time'].apply(lambda x : dt.datetime.fromisoformat(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c900bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PBLH_df_half_deg['date_time']=PBLH_df_half_deg['date_time'].apply(lambda x : dt.datetime.fromisoformat(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing PBLH data as CSV\n",
    "def readable_data(PBLH_df,latlong,filename):\n",
    "    zone_df = PBLH_df.query(str(latlong[0])+'< Lat <'+str(latlong[1])).query(str(latlong[2])+'< Long <'+str(latlong[3]))\n",
    "    def dn_convert(dn_val):\n",
    "        if dn_val == 0:\n",
    "            return 'Day'\n",
    "        else:\n",
    "            return 'Night'\n",
    "        \n",
    "    zone_df['Day/Night'] = zone_df['DN_Flag'].map(dn_convert)\n",
    "    zone_df.to_csv(filename, index = False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83572b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Enter the latitude and longitudes to query within which to query (we will use boxes set 1degree by 1degree around)\n",
    "hanle_coords = [32.28,33.28,78.46,79.46]\n",
    "merak_coords = [33.30,34.30,78.12,79.12]\n",
    "ali_coords = [32.13,33.13,79.50,80.50]\n",
    "devasthal_coords = [28.86,29.86,79.18,80.18]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47593bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We take one degree by 1degree box around Hanle and print the values that come within this box\n",
    "readable_data(PBLH_df_half_deg,hanle_coords,\"Hanle_half_deg_Jan_Jul_2012.csv\")\n",
    "readable_data(PBLH_df_point_one,hanle_coords,\"Hanle_point_one_Jan_Jul_2012.csv\")\n",
    "\n",
    "#We take one degree by 1degree box around merak and print the values that come within this box\n",
    "readable_data(PBLH_df_half_deg,merak_coords,\"merak_half_deg_Jan_Jul_2012.csv\")\n",
    "readable_data(PBLH_df_point_one,merak_coords,\"merak_point_one_Jan_Jul_2012.csv\")\n",
    "\n",
    "#We take one degree by 1degree box around ali and print the values that come within this box\n",
    "readable_data(PBLH_df_half_deg,ali_coords,\"ali_half_deg_Jan_Jul_2012.csv\")\n",
    "readable_data(PBLH_df_point_one,ali_coords,\"ali_point_one_Jan_Jul_2012.csv\")\n",
    "\n",
    "#We take one degree by 1degree box around devasthal and print the values that come within this box\n",
    "readable_data(PBLH_df_half_deg,devasthal_coords,\"devasthal_half_deg_Jan_Jul_2012.csv\")\n",
    "readable_data(PBLH_df_point_one,devasthal_coords,\"devasthal_point_one_Jan_Jul_2012.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897279f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hanle_df_night_p1 = PBLH_df_point_one[PBLH_df_point_one['DN_Flag']==1].query(str(hanle_coords[0])+'< Lat <'+str(hanle_coords[1])).query(str(hanle_coords[2])+'< Long <'+str(hanle_coords[3]))\n",
    "hanle_df_day_p1 = PBLH_df_point_one[PBLH_df_point_one['DN_Flag']==0].query(str(hanle_coords[0])+'< Lat <'+str(hanle_coords[1])).query(str(hanle_coords[2])+'< Long <'+str(hanle_coords[3]))\n",
    "\n",
    "hanle_df_day_h = PBLH_df_half_deg[PBLH_df_half_deg['DN_Flag']==0].query(str(hanle_coords[0])+'< Lat <'+str(hanle_coords[1])).query(str(hanle_coords[2])+'< Long <'+str(hanle_coords[3]))\n",
    "hanle_df_night_h = PBLH_df_half_deg[PBLH_df_half_deg['DN_Flag']==1].query(str(hanle_coords[0])+'< Lat <'+str(hanle_coords[1])).query(str(hanle_coords[2])+'< Long <'+str(hanle_coords[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "merak_df_day_p1 = PBLH_df_point_one[PBLH_df_point_one['DN_Flag']==0].query(str(merak_coords[0])+'< Lat <'+str(merak_coords[1])).query(str(merak_coords[2])+'< Long <'+str(merak_coords[3]))\n",
    "merak_df_night_p1 = PBLH_df_point_one[PBLH_df_point_one['DN_Flag']==1].query(str(merak_coords[0])+'< Lat <'+str(merak_coords[1])).query(str(merak_coords[2])+'< Long <'+str(merak_coords[3]))\n",
    "\n",
    "merak_df_day_h = PBLH_df_half_deg[PBLH_df_half_deg['DN_Flag']==0].query(str(merak_coords[0])+'< Lat <'+str(merak_coords[1])).query(str(merak_coords[2])+'< Long <'+str(merak_coords[3]))\n",
    "merak_df_night_h = PBLH_df_half_deg[PBLH_df_half_deg['DN_Flag']==1].query(str(merak_coords[0])+'< Lat <'+str(merak_coords[1])).query(str(merak_coords[2])+'< Long <'+str(merak_coords[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891832fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ali_df_day_p1 = PBLH_df_point_one[PBLH_df_point_one['DN_Flag']==0].query(str(ali_coords[0])+'< Lat <'+str(ali_coords[1])).query(str(ali_coords[2])+'< Long <'+str(ali_coords[3]))\n",
    "ali_df_night_p1 = PBLH_df_point_one[PBLH_df_point_one['DN_Flag']==1].query(str(ali_coords[0])+'< Lat <'+str(ali_coords[1])).query(str(ali_coords[2])+'< Long <'+str(ali_coords[3]))\n",
    "\n",
    "ali_df_day_h = PBLH_df_half_deg[PBLH_df_half_deg['DN_Flag']==0].query(str(ali_coords[0])+'< Lat <'+str(ali_coords[1])).query(str(ali_coords[2])+'< Long <'+str(ali_coords[3]))\n",
    "ali_df_night_h = PBLH_df_half_deg[PBLH_df_half_deg['DN_Flag']==1].query(str(ali_coords[0])+'< Lat <'+str(ali_coords[1])).query(str(ali_coords[2])+'< Long <'+str(ali_coords[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "devasthal_df_day_p1 = PBLH_df_point_one[PBLH_df_point_one['DN_Flag']==0].query(str(devasthal_coords[0])+'< Lat <'+str(devasthal_coords[1])).query(str(devasthal_coords[2])+'< Long <'+str(devasthal_coords[3]))\n",
    "devasthal_df_night_p1 = PBLH_df_point_one[PBLH_df_point_one['DN_Flag']==1].query(str(devasthal_coords[0])+'< Lat <'+str(devasthal_coords[1])).query(str(devasthal_coords[2])+'< Long <'+str(devasthal_coords[3]))\n",
    "\n",
    "devasthal_df_day_h = PBLH_df_half_deg[PBLH_df_half_deg['DN_Flag']==0].query(str(devasthal_coords[0])+'< Lat <'+str(devasthal_coords[1])).query(str(devasthal_coords[2])+'< Long <'+str(devasthal_coords[3]))\n",
    "devasthal_df_night_h = PBLH_df_half_deg[PBLH_df_half_deg['DN_Flag']==1].query(str(devasthal_coords[0])+'< Lat <'+str(devasthal_coords[1])).query(str(devasthal_coords[2])+'< Long <'+str(devasthal_coords[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IIA",
   "language": "python",
   "name": "iia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
